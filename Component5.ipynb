{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Component 5: Comprehensive Benchmark Comparison\n",
    "\n",
    "**Compares VQ, PQ, and LSH across all experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "results_dir = Path(\"Data/results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load",
   "metadata": {},
   "source": [
    "## Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Accuracy vs Efficiency\n",
    "vq_exp1 = pd.read_csv(results_dir / \"vq_accuracy_efficiency.csv\")\n",
    "pq_exp1 = pd.read_csv(results_dir / \"pq_accuracy_efficiency.csv\")\n",
    "lsh_exp1 = pd.read_csv(results_dir / \"lsh_accuracy_efficiency.csv\")\n",
    "df_exp1 = pd.concat([vq_exp1, pq_exp1, lsh_exp1], ignore_index=True)\n",
    "\n",
    "print(\"=== Experiment 1: Accuracy vs Efficiency ===\")\n",
    "display(df_exp1)\n",
    "\n",
    "# Experiment 2: Scaling with N\n",
    "vq_exp2 = pd.read_csv(results_dir / \"vq_scaling_N.csv\")\n",
    "pq_exp2 = pd.read_csv(results_dir / \"pq_scaling_N.csv\")\n",
    "lsh_exp2 = pd.read_csv(results_dir / \"lsh_scaling_N.csv\")\n",
    "df_exp2 = pd.concat([vq_exp2, pq_exp2, lsh_exp2], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== Experiment 2: Scaling with N ===\")\n",
    "display(df_exp2)\n",
    "\n",
    "# Experiment 3: Scaling with Dimensionality\n",
    "vq_exp3 = pd.read_csv(results_dir / \"vq_scaling_dim.csv\")\n",
    "pq_exp3 = pd.read_csv(results_dir / \"pq_scaling_dim.csv\")\n",
    "lsh_exp3 = pd.read_csv(results_dir / \"lsh_scaling_dim.csv\")\n",
    "df_exp3 = pd.concat([vq_exp3, pq_exp3, lsh_exp3], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== Experiment 3: Scaling with Dimensionality ===\")\n",
    "display(df_exp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot1",
   "metadata": {},
   "source": [
    "## Plot 1: Accuracy vs Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_exp1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Recall vs Candidate Ratio\n",
    "for method in [\"VQ\", \"PQ\", \"LSH\"]:\n",
    "    subset = df_exp1[df_exp1[\"method\"] == method]\n",
    "    axes[0].plot(subset[\"candidate_ratio\"], subset[\"recall_at_k\"], \n",
    "                 marker=\"o\", label=method, linewidth=2, markersize=8)\n",
    "\n",
    "axes[0].set(xlabel=\"Candidate Ratio\", ylabel=\"Recall@10\",\n",
    "            title=\"Accuracy vs Efficiency\")\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# nDCG vs Candidate Ratio\n",
    "for method in [\"VQ\", \"PQ\", \"LSH\"]:\n",
    "    subset = df_exp1[df_exp1[\"method\"] == method]\n",
    "    axes[1].plot(subset[\"candidate_ratio\"], subset[\"ndcg_at_k\"], \n",
    "                 marker=\"s\", label=method, linewidth=2, markersize=8)\n",
    "\n",
    "axes[1].set(xlabel=\"Candidate Ratio\", ylabel=\"nDCG@10\",\n",
    "            title=\"Ranking Quality\")\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Recall vs Query Time\n",
    "for method in [\"VQ\", \"PQ\", \"LSH\"]:\n",
    "    subset = df_exp1[df_exp1[\"method\"] == method]\n",
    "    axes[2].plot(subset[\"query_time\"], subset[\"recall_at_k\"], \n",
    "                 marker=\"^\", label=method, linewidth=2, markersize=8)\n",
    "\n",
    "axes[2].set(xlabel=\"Query Time (s)\", ylabel=\"Recall@10\",\n",
    "            title=\"Speed vs Accuracy\")\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"comparison_accuracy_efficiency.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot2",
   "metadata": {},
   "source": [
    "## Plot 2: Scaling with N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_exp2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Build Time vs N\n",
    "for method in [\"VQ\", \"PQ\", \"LSH\"]:\n",
    "    subset = df_exp2[df_exp2[\"method\"] == method].sort_values(\"N\")\n",
    "    ax1.plot(subset[\"N\"], subset[\"build_time\"], \n",
    "             marker=\"o\", label=method, linewidth=2, markersize=8)\n",
    "\n",
    "ax1.set(xlabel=\"N (documents)\", ylabel=\"Build Time (s)\",\n",
    "        title=\"Index Construction Time\")\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Query Time vs N\n",
    "for method in [\"VQ\", \"PQ\", \"LSH\"]:\n",
    "    subset = df_exp2[df_exp2[\"method\"] == method].sort_values(\"N\")\n",
    "    ax2.plot(subset[\"N\"], subset[\"query_time\"], \n",
    "             marker=\"s\", label=method, linewidth=2, markersize=8)\n",
    "\n",
    "ax2.set(xlabel=\"N (documents)\", ylabel=\"Query Time (s)\",\n",
    "        title=\"Query Performance\")\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"comparison_scaling_N.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot3",
   "metadata": {},
   "source": [
    "## Plot 3: Scaling with Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_exp3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Build Time vs Dim\n",
    "for method in [\"VQ\", \"PQ\", \"LSH\"]:\n",
    "    subset = df_exp3[df_exp3[\"method\"] == method].sort_values(\"dim\")\n",
    "    ax1.plot(subset[\"dim\"], subset[\"build_time\"], \n",
    "             marker=\"o\", label=method, linewidth=2, markersize=8)\n",
    "\n",
    "ax1.set(xlabel=\"Dimensionality\", ylabel=\"Build Time (s)\",\n",
    "        title=\"Index Construction vs Dimensionality\")\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Query Time vs Dim\n",
    "for method in [\"VQ\", \"PQ\", \"LSH\"]:\n",
    "    subset = df_exp3[df_exp3[\"method\"] == method].sort_values(\"dim\")\n",
    "    ax2.plot(subset[\"dim\"], subset[\"query_time\"], \n",
    "             marker=\"s\", label=method, linewidth=2, markersize=8)\n",
    "\n",
    "ax2.set(xlabel=\"Dimensionality\", ylabel=\"Query Time (s)\",\n",
    "        title=\"Query Performance vs Dimensionality\")\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"comparison_scaling_dim.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot4",
   "metadata": {},
   "source": [
    "## Plot 4: Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"VQ\", \"PQ\", \"LSH\"]\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "# Best configuration per method\n",
    "best_configs = df_exp1.loc[df_exp1.groupby(\"method\")[\"recall_at_k\"].idxmax()]\n",
    "\n",
    "# Performance at N=10,000\n",
    "perf_10k = df_exp2[df_exp2[\"N\"] == 10000]\n",
    "\n",
    "# Performance at d=200\n",
    "perf_200d = df_exp3[df_exp3[\"dim\"] == 200]\n",
    "\n",
    "def get_values(df, col):\n",
    "    return [df[df[\"method\"] == m][col].values[0] if len(df[df[\"method\"] == m]) > 0 \n",
    "            else np.nan for m in methods]\n",
    "\n",
    "best_recalls = get_values(best_configs, \"recall_at_k\")\n",
    "best_ndcgs = get_values(best_configs, \"ndcg_at_k\")\n",
    "build_10k = get_values(perf_10k, \"build_time\")\n",
    "query_200d = get_values(perf_200d, \"query_time\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Best Recall\n",
    "axes[0,0].bar(x, best_recalls, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(methods)\n",
    "axes[0,0].set_ylabel(\"Recall@10\")\n",
    "axes[0,0].set_title(\"Best Recall per Method\")\n",
    "axes[0,0].set_ylim([0, 1.0])\n",
    "axes[0,0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Best nDCG\n",
    "axes[0,1].bar(x, best_ndcgs, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].set_xticklabels(methods)\n",
    "axes[0,1].set_ylabel(\"nDCG@10\")\n",
    "axes[0,1].set_title(\"Best nDCG per Method\")\n",
    "axes[0,1].set_ylim([0, 1.0])\n",
    "axes[0,1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Build Time at N=10k\n",
    "axes[1,0].bar(x, build_10k, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels(methods)\n",
    "axes[1,0].set_ylabel(\"Build Time (s)\")\n",
    "axes[1,0].set_title(\"Build Time at N=10,000\")\n",
    "axes[1,0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Query Time at d=200\n",
    "axes[1,1].bar(x, query_200d, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels(methods)\n",
    "axes[1,1].set_ylabel(\"Query Time (s)\")\n",
    "axes[1,1].set_title(\"Query Time at d=200\")\n",
    "axes[1,1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"comparison_summary.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configuration per method\n",
    "best_configs = df_exp1.loc[df_exp1.groupby(\"method\")[\"recall_at_k\"].idxmax()]\n",
    "\n",
    "summary = best_configs[[\"method\", \"recall_at_k\", \"ndcg_at_k\", \"candidate_ratio\", \"query_time\"]].copy()\n",
    "summary.columns = [\"Method\", \"Recall@10\", \"nDCG@10\", \"Candidate Ratio\", \"Query Time (s)\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST CONFIGURATIONS (HIGHEST RECALL)\")\n",
    "print(\"=\"*70)\n",
    "display(summary.reset_index(drop=True))\n",
    "\n",
    "# Performance at N=10,000\n",
    "perf_10k = df_exp2[df_exp2[\"N\"] == 10000][[\"method\", \"build_time\", \"query_time\"]].copy()\n",
    "perf_10k.columns = [\"Method\", \"Build Time (s)\", \"Query Time (s)\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE AT N=10,000 DOCUMENTS\")\n",
    "print(\"=\"*70)\n",
    "display(perf_10k.reset_index(drop=True))\n",
    "\n",
    "# Performance at d=200\n",
    "perf_200d = df_exp3[df_exp3[\"dim\"] == 200][[\"method\", \"build_time\", \"query_time\"]].copy()\n",
    "perf_200d.columns = [\"Method\", \"Build Time (s)\", \"Query Time (s)\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE AT d=200 DIMENSIONS\")\n",
    "print(\"=\"*70)\n",
    "display(perf_200d.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insights",
   "metadata": {},
   "source": [
    "## Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print_insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best = df_exp1.loc[df_exp1.groupby(\"method\")[\"recall_at_k\"].idxmax()]\n",
    "\n",
    "print(\"\\n1. ACCURACY (Recall@10):\")\n",
    "for _, row in best.sort_values(\"recall_at_k\", ascending=False).iterrows():\n",
    "    print(f\"   {row['method']:4s}: {row['recall_at_k']:.3f}\")\n",
    "\n",
    "print(\"\\n2. RANKING QUALITY (nDCG@10):\")\n",
    "for _, row in best.sort_values(\"ndcg_at_k\", ascending=False).iterrows():\n",
    "    print(f\"   {row['method']:4s}: {row['ndcg_at_k']:.3f}\")\n",
    "\n",
    "print(\"\\n3. EFFICIENCY (Candidate Ratio):\")\n",
    "for _, row in best.sort_values(\"candidate_ratio\").iterrows():\n",
    "    print(f\"   {row['method']:4s}: {row['candidate_ratio']:.3f} ({row['candidate_ratio']*100:.1f}% of docs)\")\n",
    "\n",
    "print(\"\\n4. SPEED (Query Time):\")\n",
    "for _, row in best.sort_values(\"query_time\").iterrows():\n",
    "    print(f\"   {row['method']:4s}: {row['query_time']:.4f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "informationretrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}