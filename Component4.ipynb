{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Component 4: FAISS Library Benchmark\n",
    "\n",
    "## About FAISS\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** is a library developed by Meta AI Research for efficient similarity search and clustering of dense vectors. It's designed for billion-scale datasets and is widely used in production systems.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Multiple Index Types:**\n",
    "   - **Flat (Exact)**: Brute-force exact search (baseline)\n",
    "   - **IVF (Inverted File)**: Clustering-based (similar to our VQ)\n",
    "   - **IVFPQ**: Combines IVF with Product Quantization\n",
    "   - **HNSW**: Hierarchical Navigable Small World graphs\n",
    "   - **LSH**: Locality Sensitive Hashing\n",
    "\n",
    "2. **Optimizations:**\n",
    "   - GPU acceleration support\n",
    "   - SIMD vectorization for CPU\n",
    "   - Compressed storage (PQ, SQ)\n",
    "   - Multi-threaded search\n",
    "\n",
    "3. **Suitable For:**\n",
    "   - High-dimensional dense vectors (embeddings)\n",
    "   - Large-scale datasets (millions to billions)\n",
    "   - Real-time similarity search\n",
    "   - Image/text/audio retrieval systems\n",
    "\n",
    "### Comparison to Our Implementations:\n",
    "\n",
    "- **IndexIVFFlat**: Similar to our VQ (K-means clustering)\n",
    "- **IndexIVFPQ**: Combines clustering + product quantization (like VQ + PQ)\n",
    "- **IndexLSH**: Similar to our LSH implementation\n",
    "- **IndexFlatL2**: Exact search baseline\n",
    "\n",
    "FAISS is production-ready with extensive optimizations, making it 10-100x faster than naive implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = np.load(\"Data/processed/doc_vectors_w2v.npy\").astype('float32')\n",
    "metadata = pd.read_csv(\"Data/processed/doc_metadata.csv\")\n",
    "\n",
    "print(f\"Vectors: {doc_vectors.shape}\")\n",
    "print(f\"Metadata: {metadata.shape[0]} records\")\n",
    "print(f\"FAISS version: {faiss.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg_at_k(predicted, actual, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain.\"\"\"\n",
    "    dcg = sum([1/np.log2(i+2) for i, doc in enumerate(predicted[:k]) if doc in actual[:k]])\n",
    "    idcg = sum([1/np.log2(i+2) for i in range(min(len(actual), k))])\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def compute_recall_at_k(predicted, actual, k):\n",
    "    \"\"\"Recall@k.\"\"\"\n",
    "    return len(set(predicted[:k]) & set(actual[:k])) / k\n",
    "\n",
    "def compute_ground_truth(vectors, query_indices, k=10):\n",
    "    \"\"\"Compute exact nearest neighbors using FAISS Flat index.\"\"\"\n",
    "    d = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(vectors)\n",
    "    \n",
    "    ground_truth = {}\n",
    "    for qi in query_indices:\n",
    "        _, I = index.search(vectors[qi:qi+1], k)\n",
    "        ground_truth[qi] = I[0]\n",
    "    \n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ground_truth",
   "metadata": {},
   "source": [
    "## Precompute Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_gt",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 10\n",
    "N_QUERIES = 50\n",
    "\n",
    "query_indices = np.random.choice(len(doc_vectors), N_QUERIES, replace=False)\n",
    "\n",
    "print(\"Computing ground truth with exact search...\")\n",
    "exact_results = compute_ground_truth(doc_vectors, query_indices, TOP_K)\n",
    "print(\"✓ Ground truth computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp1",
   "metadata": {},
   "source": [
    "## Experiment 1: Accuracy vs Efficiency\n",
    "\n",
    "We test three FAISS index types with varying parameters:\n",
    "- **IndexIVFFlat**: Inverted file index (like VQ)\n",
    "- **IndexIVFPQ**: IVF with Product Quantization\n",
    "- **IndexLSH**: Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_ivf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "d = doc_vectors.shape[1]\n",
    "n_clusters = 100\n",
    "\n",
    "print(\"\\n=== Testing IndexIVFFlat (IVF) ===\")\n",
    "\n",
    "# Train quantizer\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, d, n_clusters)\n",
    "index_ivf.train(doc_vectors)\n",
    "index_ivf.add(doc_vectors)\n",
    "\n",
    "for nprobe in [1, 2, 5, 10, 20]:\n",
    "    print(f\"  Testing nprobe={nprobe}\")\n",
    "    index_ivf.nprobe = nprobe\n",
    "    \n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    query_times = []\n",
    "    \n",
    "    for qi in query_indices:\n",
    "        t0 = time.perf_counter()\n",
    "        _, I = index_ivf.search(doc_vectors[qi:qi+1], TOP_K)\n",
    "        query_times.append(time.perf_counter() - t0)\n",
    "        \n",
    "        recalls.append(compute_recall_at_k(I[0], exact_results[qi], TOP_K))\n",
    "        ndcgs.append(compute_ndcg_at_k(I[0], exact_results[qi], TOP_K))\n",
    "    \n",
    "    results.append({\n",
    "        \"method\": \"FAISS-IVF\",\n",
    "        \"index_type\": \"IndexIVFFlat\",\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"nprobe\": nprobe,\n",
    "        \"recall_at_k\": np.mean(recalls),\n",
    "        \"ndcg_at_k\": np.mean(ndcgs),\n",
    "        \"candidate_ratio\": nprobe / n_clusters,\n",
    "        \"query_time\": np.mean(query_times),\n",
    "        \"N\": len(doc_vectors),\n",
    "        \"dim\": d,\n",
    "    })\n",
    "\n",
    "df_exp1 = pd.DataFrame(results)\n",
    "print(\"\\n=== IVF Results ===\")\n",
    "display(df_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_ivfpq",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Testing IndexIVFPQ (IVF + PQ) ===\")\n",
    "\n",
    "m = 8  # number of subquantizers\n",
    "nbits = 8  # bits per subquantizer\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index_ivfpq = faiss.IndexIVFPQ(quantizer, d, n_clusters, m, nbits)\n",
    "index_ivfpq.train(doc_vectors)\n",
    "index_ivfpq.add(doc_vectors)\n",
    "\n",
    "for nprobe in [1, 2, 5, 10, 20]:\n",
    "    print(f\"  Testing nprobe={nprobe}\")\n",
    "    index_ivfpq.nprobe = nprobe\n",
    "    \n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    query_times = []\n",
    "    \n",
    "    for qi in query_indices:\n",
    "        t0 = time.perf_counter()\n",
    "        _, I = index_ivfpq.search(doc_vectors[qi:qi+1], TOP_K)\n",
    "        query_times.append(time.perf_counter() - t0)\n",
    "        \n",
    "        recalls.append(compute_recall_at_k(I[0], exact_results[qi], TOP_K))\n",
    "        ndcgs.append(compute_ndcg_at_k(I[0], exact_results[qi], TOP_K))\n",
    "    \n",
    "    results.append({\n",
    "        \"method\": \"FAISS-IVFPQ\",\n",
    "        \"index_type\": \"IndexIVFPQ\",\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"nprobe\": nprobe,\n",
    "        \"recall_at_k\": np.mean(recalls),\n",
    "        \"ndcg_at_k\": np.mean(ndcgs),\n",
    "        \"candidate_ratio\": nprobe / n_clusters,\n",
    "        \"query_time\": np.mean(query_times),\n",
    "        \"N\": len(doc_vectors),\n",
    "        \"dim\": d,\n",
    "    })\n",
    "\n",
    "df_exp1 = pd.DataFrame(results)\n",
    "print(\"\\n=== IVFPQ Results ===\")\n",
    "display(df_exp1[df_exp1[\"method\"] == \"FAISS-IVFPQ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_lsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Testing IndexLSH ===\")\n",
    "\n",
    "for nbits in [64, 128, 256, 512]:\n",
    "    print(f\"  Testing nbits={nbits}\")\n",
    "    index_lsh = faiss.IndexLSH(d, nbits)\n",
    "    index_lsh.add(doc_vectors)\n",
    "    \n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    query_times = []\n",
    "    \n",
    "    for qi in query_indices:\n",
    "        t0 = time.perf_counter()\n",
    "        _, I = index_lsh.search(doc_vectors[qi:qi+1], TOP_K)\n",
    "        query_times.append(time.perf_counter() - t0)\n",
    "        \n",
    "        recalls.append(compute_recall_at_k(I[0], exact_results[qi], TOP_K))\n",
    "        ndcgs.append(compute_ndcg_at_k(I[0], exact_results[qi], TOP_K))\n",
    "    \n",
    "    results.append({\n",
    "        \"method\": \"FAISS-LSH\",\n",
    "        \"index_type\": \"IndexLSH\",\n",
    "        \"n_clusters\": None,\n",
    "        \"nprobe\": nbits,\n",
    "        \"recall_at_k\": np.mean(recalls),\n",
    "        \"ndcg_at_k\": np.mean(ndcgs),\n",
    "        \"candidate_ratio\": 1.0,\n",
    "        \"query_time\": np.mean(query_times),\n",
    "        \"N\": len(doc_vectors),\n",
    "        \"dim\": d,\n",
    "    })\n",
    "\n",
    "df_exp1 = pd.DataFrame(results)\n",
    "print(\"\\n=== LSH Results ===\")\n",
    "display(df_exp1[df_exp1[\"method\"] == \"FAISS-LSH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Experiment 1: All FAISS Methods ===\")\n",
    "display(df_exp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp2",
   "metadata": {},
   "source": [
    "## Experiment 2: Scaling with N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LIST = [1000, 2000, 5000, 10000, min(20000, len(doc_vectors))]\n",
    "TEST_QUERIES = 10\n",
    "BEST_NPROBE = 5\n",
    "\n",
    "scaling_results = []\n",
    "\n",
    "for N in N_LIST:\n",
    "    print(f\"\\nTesting N={N}\")\n",
    "    X = doc_vectors[:N]\n",
    "    test_idx = np.random.choice(N, TEST_QUERIES, replace=False)\n",
    "    \n",
    "    # IVFFlat\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, min(100, N//10))\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    index.train(X)\n",
    "    index.add(X)\n",
    "    build_t = time.perf_counter() - t0\n",
    "    \n",
    "    index.nprobe = BEST_NPROBE\n",
    "    q_times = []\n",
    "    for qi in test_idx:\n",
    "        t1 = time.perf_counter()\n",
    "        index.search(X[qi:qi+1], TOP_K)\n",
    "        q_times.append(time.perf_counter() - t1)\n",
    "    \n",
    "    scaling_results.append({\n",
    "        \"method\": \"FAISS-IVF\",\n",
    "        \"N\": N,\n",
    "        \"dim\": d,\n",
    "        \"build_time\": build_t,\n",
    "        \"query_time\": np.mean(q_times),\n",
    "    })\n",
    "    \n",
    "    # IVFPQ\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "    index_pq = faiss.IndexIVFPQ(quantizer, d, min(100, N//10), 8, 8)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    index_pq.train(X)\n",
    "    index_pq.add(X)\n",
    "    build_t = time.perf_counter() - t0\n",
    "    \n",
    "    index_pq.nprobe = BEST_NPROBE\n",
    "    q_times = []\n",
    "    for qi in test_idx:\n",
    "        t1 = time.perf_counter()\n",
    "        index_pq.search(X[qi:qi+1], TOP_K)\n",
    "        q_times.append(time.perf_counter() - t1)\n",
    "    \n",
    "    scaling_results.append({\n",
    "        \"method\": \"FAISS-IVFPQ\",\n",
    "        \"N\": N,\n",
    "        \"dim\": d,\n",
    "        \"build_time\": build_t,\n",
    "        \"query_time\": np.mean(q_times),\n",
    "    })\n",
    "    \n",
    "    # LSH\n",
    "    index_lsh = faiss.IndexLSH(d, 128)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    index_lsh.add(X)\n",
    "    build_t = time.perf_counter() - t0\n",
    "    \n",
    "    q_times = []\n",
    "    for qi in test_idx:\n",
    "        t1 = time.perf_counter()\n",
    "        index_lsh.search(X[qi:qi+1], TOP_K)\n",
    "        q_times.append(time.perf_counter() - t1)\n",
    "    \n",
    "    scaling_results.append({\n",
    "        \"method\": \"FAISS-LSH\",\n",
    "        \"N\": N,\n",
    "        \"dim\": d,\n",
    "        \"build_time\": build_t,\n",
    "        \"query_time\": np.mean(q_times),\n",
    "    })\n",
    "\n",
    "df_exp2 = pd.DataFrame(scaling_results)\n",
    "print(\"\\n=== Experiment 2: Scaling with N ===\")\n",
    "display(df_exp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp3",
   "metadata": {},
   "source": [
    "## Experiment 3: Scaling with Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_LIST = [50, 100, 200]\n",
    "N_SAMPLE = 10000\n",
    "TEST_QUERIES = 10\n",
    "\n",
    "dim_results = []\n",
    "X_sample = doc_vectors[:N_SAMPLE]\n",
    "\n",
    "for dim in DIM_LIST:\n",
    "    print(f\"\\nTesting d={dim}\")\n",
    "    X = X_sample[:, :dim]\n",
    "    test_idx = np.random.choice(N_SAMPLE, TEST_QUERIES, replace=False)\n",
    "    \n",
    "    # IVFFlat\n",
    "    quantizer = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dim, 100)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    index.train(X)\n",
    "    index.add(X)\n",
    "    build_t = time.perf_counter() - t0\n",
    "    \n",
    "    index.nprobe = BEST_NPROBE\n",
    "    q_times = []\n",
    "    for qi in test_idx:\n",
    "        t1 = time.perf_counter()\n",
    "        index.search(X[qi:qi+1], TOP_K)\n",
    "        q_times.append(time.perf_counter() - t1)\n",
    "    \n",
    "    dim_results.append({\n",
    "        \"method\": \"FAISS-IVF\",\n",
    "        \"N\": N_SAMPLE,\n",
    "        \"dim\": dim,\n",
    "        \"build_time\": build_t,\n",
    "        \"query_time\": np.mean(q_times),\n",
    "    })\n",
    "    \n",
    "    # IVFPQ\n",
    "    m = max(2, dim // 25)\n",
    "    quantizer = faiss.IndexFlatL2(dim)\n",
    "    index_pq = faiss.IndexIVFPQ(quantizer, dim, 100, m, 8)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    index_pq.train(X)\n",
    "    index_pq.add(X)\n",
    "    build_t = time.perf_counter() - t0\n",
    "    \n",
    "    index_pq.nprobe = BEST_NPROBE\n",
    "    q_times = []\n",
    "    for qi in test_idx:\n",
    "        t1 = time.perf_counter()\n",
    "        index_pq.search(X[qi:qi+1], TOP_K)\n",
    "        q_times.append(time.perf_counter() - t1)\n",
    "    \n",
    "    dim_results.append({\n",
    "        \"method\": \"FAISS-IVFPQ\",\n",
    "        \"N\": N_SAMPLE,\n",
    "        \"dim\": dim,\n",
    "        \"build_time\": build_t,\n",
    "        \"query_time\": np.mean(q_times),\n",
    "    })\n",
    "    \n",
    "    # LSH\n",
    "    nbits = dim * 2\n",
    "    index_lsh = faiss.IndexLSH(dim, nbits)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    index_lsh.add(X)\n",
    "    build_t = time.perf_counter() - t0\n",
    "    \n",
    "    q_times = []\n",
    "    for qi in test_idx:\n",
    "        t1 = time.perf_counter()\n",
    "        index_lsh.search(X[qi:qi+1], TOP_K)\n",
    "        q_times.append(time.perf_counter() - t1)\n",
    "    \n",
    "    dim_results.append({\n",
    "        \"method\": \"FAISS-LSH\",\n",
    "        \"N\": N_SAMPLE,\n",
    "        \"dim\": dim,\n",
    "        \"build_time\": build_t,\n",
    "        \"query_time\": np.mean(q_times),\n",
    "    })\n",
    "\n",
    "df_exp3 = pd.DataFrame(dim_results)\n",
    "print(\"\\n=== Experiment 3: Scaling with Dimensionality ===\")\n",
    "display(df_exp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"Data/results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "df_exp1.to_csv(f\"{results_dir}/faiss_accuracy_efficiency.csv\", index=False)\n",
    "df_exp2.to_csv(f\"{results_dir}/faiss_scaling_N.csv\", index=False)\n",
    "df_exp3.to_csv(f\"{results_dir}/faiss_scaling_dim.csv\", index=False)\n",
    "\n",
    "print(\"✓ All FAISS results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_exp1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot Recall vs Candidate Ratio for Experiment 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_exp1, x=\"candidate_ratio\", y=\"recall_at_k\", hue=\"method\", marker=\"o\")\n",
    "plt.title(\"FAISS: Recall@K vs Candidate Ratio\")\n",
    "plt.xlabel(\"Candidate Ratio\")\n",
    "plt.ylabel(\"Recall@K\")\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Method\")\n",
    "plt.show()"
   ]
  }
  
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }