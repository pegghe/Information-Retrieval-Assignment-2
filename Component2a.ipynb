{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Component 2: Vector Quantization Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(\"Components\")\n",
    "from vector_quantization import fit_kmeans, vq_query\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = np.load(\"Data/processed/doc_vectors_w2v.npy\")\n",
    "metadata = pd.read_csv(\"Data/processed/doc_metadata.csv\")\n",
    "\n",
    "print(f\"Vectors: {doc_vectors.shape}\")\n",
    "print(f\"Metadata: {metadata.shape[0]} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg_at_k(predicted, actual, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain: rewards correct ordering.\"\"\"\n",
    "    dcg = sum([1/np.log2(i+2) for i, doc in enumerate(predicted[:k]) if doc in actual[:k]])\n",
    "    idcg = sum([1/np.log2(i+2) for i in range(min(len(actual), k))])\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def compute_recall_at_k(predicted, actual, k):\n",
    "    \"\"\"Recall@k: fraction of true top-k found in predicted top-k.\"\"\"\n",
    "    return len(set(predicted[:k]) & set(actual[:k])) / k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp1",
   "metadata": {},
   "source": [
    "## Experiment 1: Accuracy vs Efficiency (vary n_probes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 10\n",
    "N_QUERIES = 50\n",
    "C = 200\n",
    "\n",
    "query_indices = np.random.choice(len(doc_vectors), N_QUERIES, replace=False)\n",
    "\n",
    "# Ground truth: exact nearest neighbors\n",
    "exact_results = {}\n",
    "for qi in query_indices:\n",
    "    dists = euclidean_distances(doc_vectors[qi:qi+1], doc_vectors).ravel()\n",
    "    exact_results[qi] = np.argsort(dists)[:TOP_K]\n",
    "\n",
    "# Fit KMeans once\n",
    "print(\"Fitting KMeans...\")\n",
    "kmeans, assignments = fit_kmeans(doc_vectors, k=C)\n",
    "\n",
    "results = []\n",
    "for n_probes in [1, 2, 5, 10, 20]:\n",
    "    print(f\"Testing n_probes={n_probes}\")\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    candidate_ratios = []\n",
    "    query_times = []\n",
    "    \n",
    "    for qi in query_indices:\n",
    "        # Measure query time\n",
    "        t0 = time.perf_counter()\n",
    "        indices, _ = vq_query(doc_vectors[qi], doc_vectors, kmeans, assignments, \n",
    "                              top_n=TOP_K, n_probes=n_probes)\n",
    "        query_times.append(time.perf_counter() - t0)\n",
    "        \n",
    "        # Metrics\n",
    "        recalls.append(compute_recall_at_k(indices, exact_results[qi], TOP_K))\n",
    "        ndcgs.append(compute_ndcg_at_k(indices, exact_results[qi], TOP_K))\n",
    "        \n",
    "        # Candidate ratio\n",
    "        cluster_ids = np.argsort(euclidean_distances(\n",
    "            doc_vectors[qi:qi+1], kmeans.cluster_centers_\n",
    "        ).ravel())[:n_probes]\n",
    "        n_candidates = np.sum(np.isin(assignments, cluster_ids))\n",
    "        candidate_ratios.append(n_candidates / len(doc_vectors))\n",
    "    \n",
    "    results.append({\n",
    "        \"method\": \"VQ\",\n",
    "        \"n_probes\": n_probes,\n",
    "        \"c_clusters\": C,\n",
    "        \"recall_at_k\": np.mean(recalls),\n",
    "        \"ndcg_at_k\": np.mean(ndcgs),\n",
    "        \"candidate_ratio\": np.mean(candidate_ratios),\n",
    "        \"query_time\": np.mean(query_times),\n",
    "        \"N\": len(doc_vectors),\n",
    "        \"dim\": doc_vectors.shape[1],\n",
    "    })\n",
    "\n",
    "df_exp1 = pd.DataFrame(results)\n",
    "print(\"\\n=== Experiment 1: Accuracy vs Efficiency ===\")\n",
    "display(df_exp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp2",
   "metadata": {},
   "source": [
    "## Experiment 2: Scaling with N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LIST = [1000, 2000, 5000, 10000, min(20000, len(doc_vectors))]\n",
    "TEST_QUERIES = 10\n",
    "BEST_N_PROBES = 5\n",
    "\n",
    "scaling_results = []\n",
    "\n",
    "for N in N_LIST:\n",
    "    print(f\"Testing N={N}\")\n",
    "    X = doc_vectors[:N]\n",
    "    test_idx = np.random.choice(N, TEST_QUERIES, replace=False)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    kmeans, assignments = fit_kmeans(X, k=min(200, N//10))\n",
    "    build_t = time.perf_counter() - t0\n",
    "    \n",
    "    q_times = []\n",
    "    for qi in test_idx:\n",
    "        t1 = time.perf_counter()\n",
    "        vq_query(X[qi], X, kmeans, assignments, top_n=TOP_K, n_probes=BEST_N_PROBES)\n",
    "        q_times.append(time.perf_counter() - t1)\n",
    "    \n",
    "    scaling_results.append({\n",
    "        \"method\": \"VQ\",\n",
    "        \"N\": N,\n",
    "        \"dim\": X.shape[1],\n",
    "        \"build_time\": build_t,\n",
    "        \"query_time\": np.mean(q_times),\n",
    "    })\n",
    "\n",
    "df_exp2 = pd.DataFrame(scaling_results)\n",
    "print(\"\\n=== Experiment 2: Scaling with N ===\")\n",
    "display(df_exp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp3",
   "metadata": {},
   "source": [
    "## Experiment 3: Scaling with Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_LIST = [50, 100, 200]\n",
    "N_SAMPLE = 10000\n",
    "TEST_QUERIES = 10\n",
    "\n",
    "dim_results = []\n",
    "X_sample = doc_vectors[:N_SAMPLE]\n",
    "\n",
    "for d in DIM_LIST:\n",
    "    print(f\"Testing d={d}\")\n",
    "    X = X_sample[:, :d]\n",
    "    test_idx = np.random.choice(N_SAMPLE, TEST_QUERIES, replace=False)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    kmeans, assignments = fit_kmeans(X, k=200)\n",
    "    build_t = time.perf_counter() - t0\n",
    "    \n",
    "    q_times = []\n",
    "    for qi in test_idx:\n",
    "        t1 = time.perf_counter()\n",
    "        vq_query(X[qi], X, kmeans, assignments, top_n=TOP_K, n_probes=BEST_N_PROBES)\n",
    "        q_times.append(time.perf_counter() - t1)\n",
    "    \n",
    "    dim_results.append({\n",
    "        \"method\": \"VQ\",\n",
    "        \"N\": N_SAMPLE,\n",
    "        \"dim\": d,\n",
    "        \"build_time\": build_t,\n",
    "        \"query_time\": np.mean(q_times),\n",
    "    })\n",
    "\n",
    "df_exp3 = pd.DataFrame(dim_results)\n",
    "print(\"\\n=== Experiment 3: Scaling with Dimensionality ===\")\n",
    "display(df_exp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"Data/results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "df_exp1.to_csv(f\"{results_dir}/vq_accuracy_efficiency.csv\", index=False)\n",
    "df_exp2.to_csv(f\"{results_dir}/vq_scaling_N.csv\", index=False)\n",
    "df_exp3.to_csv(f\"{results_dir}/vq_scaling_dim.csv\", index=False)\n",
    "\n",
    "print(\"âœ“ All VQ results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Recall vs Candidate Ratio\n",
    "axes[0,0].plot(df_exp1[\"candidate_ratio\"], df_exp1[\"recall_at_k\"], 'o-', linewidth=2)\n",
    "axes[0,0].set(xlabel=\"Candidate Ratio\", ylabel=\"Recall@10\", \n",
    "              title=\"VQ: Accuracy vs Efficiency\")\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# nDCG vs Candidate Ratio\n",
    "axes[0,1].plot(df_exp1[\"candidate_ratio\"], df_exp1[\"ndcg_at_k\"], 's-', \n",
    "               linewidth=2, color='orange')\n",
    "axes[0,1].set(xlabel=\"Candidate Ratio\", ylabel=\"nDCG@10\", \n",
    "              title=\"VQ: Ranking Quality\")\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Scaling with N\n",
    "ax2 = axes[1,0]\n",
    "ax2.plot(df_exp2[\"N\"], df_exp2[\"build_time\"], 'o-', label=\"Build\", linewidth=2)\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(df_exp2[\"N\"], df_exp2[\"query_time\"], 's-', color='orange', \n",
    "              label=\"Query\", linewidth=2)\n",
    "ax2.set(xlabel=\"N (documents)\", ylabel=\"Build Time (s)\", \n",
    "        title=\"VQ: Scaling with N\")\n",
    "ax2_twin.set_ylabel(\"Query Time (s)\")\n",
    "ax2.legend(loc='upper left')\n",
    "ax2_twin.legend(loc='upper right')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Scaling with Dim\n",
    "ax3 = axes[1,1]\n",
    "ax3.plot(df_exp3[\"dim\"], df_exp3[\"build_time\"], 'o-', label=\"Build\", linewidth=2)\n",
    "ax3_twin = ax3.twinx()\n",
    "ax3_twin.plot(df_exp3[\"dim\"], df_exp3[\"query_time\"], 's-', color='orange', \n",
    "              label=\"Query\", linewidth=2)\n",
    "ax3.set(xlabel=\"Dimensionality\", ylabel=\"Build Time (s)\", \n",
    "        title=\"VQ: Scaling with Dimensions\")\n",
    "ax3_twin.set_ylabel(\"Query Time (s)\")\n",
    "ax3.legend(loc='upper left')\n",
    "ax3_twin.legend(loc='upper right')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/vq_benchmark_summary.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "informationretrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}