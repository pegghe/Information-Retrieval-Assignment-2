{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f92488c",
   "metadata": {},
   "source": [
    "# Component 1: Transforming documents to dense vectors (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d546390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Tokenizer in /opt/anaconda3/envs/ir-faiss/lib/python3.10/site-packages (3.5.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('Components')\n",
    "from Tokenizer import tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229019bc",
   "metadata": {},
   "source": [
    "## Loading Dataset and Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b890a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Loading datasets...\n",
      "  ✓ Loaded 1770 movies from 1970s\n",
      "  ✓ Loaded 2338 movies from 1980s\n",
      "  ✓ Loaded 3105 movies from 1990s\n",
      "  ✓ Loaded 4416 movies from 2000s\n",
      "  ✓ Loaded 4960 movies from 2010s\n",
      "  ✓ Loaded 1241 movies from 2020s\n"
     ]
    }
   ],
   "source": [
    "# Load all movie datasets\n",
    "print(\"\\n[1/5] Loading datasets...\")\n",
    "dataframes = []\n",
    "for decade in ['1970s', '1980s', '1990s', '2000s', '2010s', '2020s']:\n",
    "    df = pd.read_csv(f'data/{decade}-movies.csv')\n",
    "    df['decade'] = decade\n",
    "    dataframes.append(df)\n",
    "    print(f\"  ✓ Loaded {len(df)} movies from {decade}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "913fbc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total movies loaded: 17830\n"
     ]
    }
   ],
   "source": [
    "# Combine all movies\n",
    "all_movies = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"\\nTotal movies loaded: {len(all_movies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d518e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] Data Exploration...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Dataset Info:\n",
      "  - Columns: ['title', 'image', 'plot', 'decade']\n",
      "  - Shape: (17830, 4)\n",
      "  - Missing values: {'title': 0, 'image': 0, 'plot': 0, 'decade': 0}\n",
      "\n",
      "\n",
      "First 3 movies:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. 'Gator Bait (1970s)\n",
      "   Plot: The film follows a barefoot poacher named Desiree Thibodeau who lives deep in the swampland. Ben Bracken and Deputy Billy Boy find Desiree trapping al...\n",
      "\n",
      "2. ...And Justice for All (film) (1970s)\n",
      "   Plot: Arthur Kirkland, a Baltimore defense attorney, is in jail on a contempt of court charge after punching Judge Henry T. Fleming while arguing the case o...\n",
      "\n",
      "3. 10 (1979 film) (1970s)\n",
      "   Plot: During a surprise 42nd birthday party for the wealthy and famous composer George Webber thrown by his actress girlfriend Samantha Taylor, George finds...\n",
      "\n",
      "\n",
      "Movies per decade:\n",
      "decade\n",
      "1970s    1770\n",
      "1980s    2338\n",
      "1990s    3105\n",
      "2000s    4416\n",
      "2010s    4960\n",
      "2020s    1241\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "print(\"\\n[2/5] Data Exploration...\")\n",
    "print(\"-\"*80)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"  - Columns: {list(all_movies.columns)}\")\n",
    "print(f\"  - Shape: {all_movies.shape}\")\n",
    "print(f\"  - Missing values: {all_movies.isnull().sum().to_dict()}\")\n",
    "\n",
    "print(\"\\n\\nFirst 3 movies:\")\n",
    "print(\"-\"*80)\n",
    "for i in range(3):\n",
    "    movie = all_movies.iloc[i]\n",
    "    print(f\"\\n{i+1}. {movie['title']} ({movie['decade']})\")\n",
    "    plot_preview = movie['plot'][:150] + \"...\" if len(movie['plot']) > 150 else movie['plot']\n",
    "    print(f\"   Plot: {plot_preview}\")\n",
    "\n",
    "print(\"\\n\\nMovies per decade:\")\n",
    "print(all_movies['decade'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1b9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Plot length statistics:\n",
      "  - Mean: 2700 characters\n",
      "  - Median: 2867 characters\n",
      "  - Min: 3 characters\n",
      "  - Max: 66145 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nPlot length statistics:\")\n",
    "all_movies['plot_length'] = all_movies['plot'].str.len()\n",
    "print(f\"  - Mean: {all_movies['plot_length'].mean():.0f} characters\")\n",
    "print(f\"  - Median: {all_movies['plot_length'].median():.0f} characters\")\n",
    "print(f\"  - Min: {all_movies['plot_length'].min():.0f} characters\")\n",
    "print(f\"  - Max: {all_movies['plot_length'].max():.0f} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7dc62",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31504eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Tokenizing documents...\n",
      "Processing movie titles and plots...\n",
      "Using NLTK tokenization with stemming (Porter) for optimal IR performance...\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all documents\n",
    "print(\"\\n[3/5] Tokenizing documents...\")\n",
    "print(\"Processing movie titles and plots...\")\n",
    "print(\"Using NLTK tokenization with stemming (Porter) for optimal IR performance...\")\n",
    "\n",
    "# Tokenize each movie (title + plot)\n",
    "# For indexing: use stemming and remove stopwords\n",
    "all_movies['tokens'] = all_movies.apply(\n",
    "    lambda row: tokenize(str(row['title']) + ' ' + str(row['plot']), \n",
    "                        remove_stopwords=True, \n",
    "                        apply_stemming=True),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3471e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 17830 documents\n",
      "  ✓ Total tokens: 4,577,616\n",
      "\n",
      "[4/5] Tokenization Analysis...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Count total tokens\n",
    "total_tokens = sum(len(tokens) for tokens in all_movies['tokens'])\n",
    "print(f\"  ✓ Processed {len(all_movies)} documents\")\n",
    "print(f\"  ✓ Total tokens: {total_tokens:,}\")\n",
    "\n",
    "# Tokenization analysis\n",
    "print(\"\\n[4/5] Tokenization Analysis...\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5e7912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens per document statistics:\n",
      "  - Mean: 256.7 tokens\n",
      "  - Median: 271.0 tokens\n",
      "  - Min: 4 tokens\n",
      "  - Max: 6243 tokens\n"
     ]
    }
   ],
   "source": [
    "# Token count per document\n",
    "all_movies['token_count'] = all_movies['tokens'].apply(len)\n",
    "print(f\"\\nTokens per document statistics:\")\n",
    "print(f\"  - Mean: {all_movies['token_count'].mean():.1f} tokens\")\n",
    "print(f\"  - Median: {all_movies['token_count'].median():.1f} tokens\")\n",
    "print(f\"  - Min: {all_movies['token_count'].min()} tokens\")\n",
    "print(f\"  - Max: {all_movies['token_count'].max()} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5472092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building vocabulary...\n",
      "  ✓ Unique tokens in vocabulary: 65,429\n",
      "\n",
      "Top 20 most frequent tokens:\n",
      "  kill                 : 24,252 occurrences\n",
      "  find                 : 22,815 occurrences\n",
      "  film                 : 19,636 occurrences\n",
      "  take                 : 18,517 occurrences\n",
      "  tell                 : 17,799 occurrences\n",
      "  one                  : 17,778 occurrences\n",
      "  get                  : 17,059 occurrences\n",
      "  leav                 : 16,972 occurrences\n",
      "  back                 : 14,523 occurrences\n",
      "  return               : 13,432 occurrences\n",
      "  two                  : 13,174 occurrences\n",
      "  friend               : 12,973 occurrences\n",
      "  home                 : 12,717 occurrences\n",
      "  tri                  : 12,515 occurrences\n",
      "  hous                 : 12,427 occurrences\n",
      "  father               : 12,423 occurrences\n",
      "  new                  : 12,150 occurrences\n",
      "  later                : 12,125 occurrences\n",
      "  make                 : 12,038 occurrences\n",
      "  go                   : 11,718 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "print(f\"\\nBuilding vocabulary...\")\n",
    "vocabulary = set()\n",
    "for tokens in all_movies['tokens']:\n",
    "    vocabulary.update(tokens)\n",
    "print(f\"  ✓ Unique tokens in vocabulary: {len(vocabulary):,}\")\n",
    "\n",
    "# Most common tokens\n",
    "from collections import Counter\n",
    "all_tokens_flat = [token for tokens in all_movies['tokens'] for token in tokens]\n",
    "token_freq = Counter(all_tokens_flat)\n",
    "print(f\"\\nTop 20 most frequent tokens:\")\n",
    "for token, count in token_freq.most_common(20):\n",
    "    print(f\"  {token:20s} : {count:6,} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9599bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] Sample tokenized documents:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. 'Gator Bait (1970s)\n",
      "   Original plot length: 521 chars\n",
      "   Tokens (59): ['bait', 'film', 'follow', 'barefoot', 'poacher', 'name', 'desire', 'thibodeau', 'live', 'deep', 'swampland', 'ben', 'bracken', 'deputi', 'billi']...\n",
      "\n",
      "2. ...And Justice for All (film) (1970s)\n",
      "   Original plot length: 5112 chars\n",
      "   Tokens (483): ['justic', 'film', 'arthur', 'kirkland', 'baltimor', 'defens', 'attorney', 'jail', 'contempt', 'court', 'charg', 'punch', 'judg', 'henri', 'fleme']...\n",
      "\n",
      "3. 10 (1979 film) (1970s)\n",
      "   Original plot length: 3070 chars\n",
      "   Tokens (284): ['10', '1979', 'film', 'surpris', '42nd', 'birthday', 'parti', 'wealthi', 'famou', 'compos', 'georg', 'webber', 'thrown', 'actress', 'girlfriend']...\n"
     ]
    }
   ],
   "source": [
    "# Show sample\n",
    "print(\"\\n[5/5] Sample tokenized documents:\")\n",
    "print(\"-\"*80)\n",
    "for i in range(3):\n",
    "    movie = all_movies.iloc[i]\n",
    "    print(f\"\\n{i+1}. {movie['title']} ({movie['decade']})\")\n",
    "    print(f\"   Original plot length: {len(movie['plot'])} chars\")\n",
    "    print(f\"   Tokens ({len(movie['tokens'])}): {movie['tokens'][:15]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df66d4c",
   "metadata": {},
   "source": [
    "## To dense Vectors using Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d580085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/6] Training Word2Vec model on tokenized documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Word2Vec model trained.\n",
      "  - Vocabulary size: 29,086 tokens\n",
      "  - Embedding dimension: 200\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec training\n",
    "print(\"\\n[6/6] Training Word2Vec model on tokenized documents...\")\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Hyper-parameters (scelta ragionevole per l'assignment)\n",
    "VECTOR_SIZE = 200   # dimensione embedding documento (puoi citare nel report)\n",
    "WINDOW = 5          # contesto locale\n",
    "MIN_COUNT = 5       # ignora parole troppo rare\n",
    "WORKERS = 4         # thread (puoi adattare alla tua macchina)\n",
    "SG = 1              # 1 = Skip-gram, 0 = CBOW\n",
    "\n",
    "sentences = list(all_movies['tokens'])  # lista di liste di token\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=WORKERS,\n",
    "    sg=SG\n",
    ")\n",
    "\n",
    "print(\"  ✓ Word2Vec model trained.\")\n",
    "print(f\"  - Vocabulary size: {len(w2v_model.wv.key_to_index):,} tokens\")\n",
    "print(f\"  - Embedding dimension: {w2v_model.vector_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/6] Computing document-level embeddings...\n",
      "  ✓ Document embeddings computed.\n",
      "  - Shape of doc_matrix: (17830, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "print(\"\\n[7/6] Computing document-level embeddings...\")\n",
    "\n",
    "def document_embedding(tokens, model, vector_size):\n",
    "    \"\"\"\n",
    "    Compute a document embedding as the mean of its token embeddings.\n",
    "    \n",
    "    Args:\n",
    "        tokens: list of string tokens for the document\n",
    "        model: trained gensim Word2Vec model\n",
    "        vector_size: embedding dimension (int)\n",
    "        \n",
    "    Returns:\n",
    "        1D numpy array of shape (vector_size,)\n",
    "    \"\"\"\n",
    "    word_vectors = model.wv\n",
    "    # prendi solo i token presenti nel vocabolario\n",
    "    valid_tokens = [t for t in tokens if t in word_vectors.key_to_index]\n",
    "    \n",
    "    if not valid_tokens:\n",
    "        # documento senza token noti → vettore nullo\n",
    "        return np.zeros(vector_size, dtype=np.float32)\n",
    "    \n",
    "    vecs = np.vstack([word_vectors[t] for t in valid_tokens])\n",
    "    return vecs.mean(axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "doc_vectors = []\n",
    "for tokens in all_movies['tokens']:\n",
    "    doc_vec = document_embedding(tokens, w2v_model, VECTOR_SIZE)\n",
    "    doc_vectors.append(doc_vec)\n",
    "\n",
    "doc_matrix = np.vstack(doc_vectors)  # shape: (n_docs, VECTOR_SIZE)\n",
    "print(f\"  ✓ Document embeddings computed.\")\n",
    "print(f\"  - Shape of doc_matrix: {doc_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2d791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/6] L2-normalizing document embeddings (for cosine similarity)...\n",
      "  ✓ Normalization done.\n",
      "  - Example norm before: 1.5168\n",
      "  - Example norm after:  1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[8/6] L2-normalizing document embeddings (for cosine similarity)...\")\n",
    "\n",
    "# Evita divisione per zero\n",
    "norms = norm(doc_matrix, axis=1, keepdims=True)\n",
    "norms[norms == 0.0] = 1.0  # i vettori zero restano zero\n",
    "\n",
    "doc_matrix_normalized = doc_matrix / norms\n",
    "\n",
    "print(\"  ✓ Normalization done.\")\n",
    "print(f\"  - Example norm before: {norm(doc_matrix[0]):.4f}\")\n",
    "print(f\"  - Example norm after:  {norm(doc_matrix_normalized[0]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9/6] Saving document embeddings to disk...\n",
      "  ✓ Saved:\n",
      "    - data/processed/doc_vectors_w2v.npy (document vectors)\n",
      "    - data/processed/doc_metadata.csv (titles and metadata)\n"
     ]
    }
   ],
   "source": [
    "# Save for later components (Vector Quantization, LSH, etc.)\n",
    "print(\"\\n[9/6] Saving document embeddings to disk...\")\n",
    "\n",
    "np.save(\"data/processed/doc_vectors_w2v.npy\", doc_matrix_normalized)\n",
    "all_movies[['title', 'decade']].to_csv(\"data/processed/doc_metadata.csv\", index=False)\n",
    "\n",
    "print(\"  ✓ Saved:\")\n",
    "print(\"    - data/processed/doc_vectors_w2v.npy (document vectors)\")\n",
    "print(\"    - data/processed/doc_metadata.csv (titles and metadata)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir-faiss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
